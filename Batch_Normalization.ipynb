{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# <h1 style=\"background-color: #f0f0f0; padding: 10px 20px; border-radius: 10px; display: inline-block;\">Batch Normalization</h1>\n"
      ],
      "metadata": {
        "id": "UgatDFYZD-7D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Objective:** The objective of this assignment is to assess students' understanding of batch normalization in\n",
        "artificial neural networks (ANN) and its impact on training performance.\n",
        "\n",
        "**Theory and Concepts**\n",
        "\n",
        "Q1. Explain the concept of batch normalization in the context of Artificial Neural Network.\n",
        "\n",
        "Q2. Describe the benefits of using batch normalization during training.\n",
        "\n",
        "Q3. Discuss the working principle of batch normalization, including the normalization step and the learnable parameters."
      ],
      "metadata": {
        "id": "RiQppnhiExsV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q1. Explain the concept of batch normalization in the context of Artificial Neural Network.\n"
      ],
      "metadata": {
        "id": "IQdWL9eTEx45"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batch Normalization is a very useful concept which is used in the Neural Network. It is introduced in 2015. Batch Normalization is genrally make the training of neurone fast and so the model will become stable.It gives the good accuracy for the model in lesser ephoches. It is apllied in the layers of neural networks to normalize the data, which is the input for another layer.\n",
        "\n",
        "It is usually introduced for solving the problem of the internal covariate shift.\n",
        "\n",
        "*\" We define internal covarient shift as the change in the distribution of the network activation due to the change in the parameters during training \"\n",
        "      -Resharch paper of Batch Normalization.*\n",
        "\n",
        "In neural networks the output of every layer is become the input of the next layer.However we put the normalized data in our initial inputs but it become crucial as while in move from the various layers so the Batch Normlization make it normalized data, which has 0 mean and 1 std."
      ],
      "metadata": {
        "id": "0iPiwJL2Ex-w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2. Describe the benefits of using batch normalization during training.\n"
      ],
      "metadata": {
        "id": "kgQAl7X0EyAU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are some benifits of using the Batch Normalization technique during the training of our model.;\n",
        "\n",
        "- Batch Normilazation make training process fast\n",
        "- Model will be stable after using batch normalization.\n",
        "- Lesser EPCHOES required for acheving the accuracy which will reduce the computetional cost.\n",
        "- Learning rate of the neurones will high by the batch normalization.\n",
        "- It is a solution for Covarient Shift problem."
      ],
      "metadata": {
        "id": "PRgRGRTkK59i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3. Discuss the working principle of batch normalization, including the normalization step and the learnable parameters."
      ],
      "metadata": {
        "id": "iVykoZ86K6UC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Compute Mini-Batch Mean and Variance:**\n",
        "   - Calculate the mean and standard deviation for each feature in the mini-batch.\n",
        "\n",
        "2. **Normalize the Inputs:**\n",
        "   - Normalize each feature by subtracting the mean and dividing by the standard deviation:\n",
        "     \\[ y_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\]\n",
        "\n",
        "3. **Scale and Shift:**\n",
        "   - Introduce learnable parameters (\\(\\gamma\\) and \\(\\beta\\)) for each feature.\n",
        "   - Scale and shift the normalized values:\n",
        "     \\[ z_i = \\gamma \\cdot y_i + \\beta \\]\n",
        "\n",
        "4. **Learnable Parameters:**\n",
        "   - \\(\\gamma\\) controls scaling, and \\(\\beta\\) controls shifting.\n",
        "   - These parameters are learned during training."
      ],
      "metadata": {
        "id": "RrwQmDDuK6fp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q. Impementation\n",
        "\n",
        "1. Choose a dataset of your choice (e.g., MNIST, CIAR-0) and preprocess it.\n",
        "2. Implement a simple feedforward neural network using any deep learning framework/library (e.g. Tensorlow, xyTorch)\n",
        "3. Train the neural network on the chosen dataset without using batch normalization.\n",
        "4. Implement batch normalization layers in the neural network and train the model again.\n",
        "5. Compare the training and validation performance (e.g., accuracy, loss) between the models with and without batch normalization.\n",
        "6. Discuss the impact of batch normalization on the training process and the performance of the neural network."
      ],
      "metadata": {
        "id": "yK7fk0SpGW_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing important libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n"
      ],
      "metadata": {
        "id": "0I4J4JtjHemu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading dataset\n",
        "from tensorflow.keras.datasets import mnist"
      ],
      "metadata": {
        "id": "z0ErzzJlIHlG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        " (x_train ,y_train), (x_test,y_test) = mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1CYrvfxJRMa",
        "outputId": "4de3872a-89f3-465b-f770-3c2d6b5d672b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "423Qe3kVJkKo",
        "outputId": "e7467bc4-0e7f-4c30-c3ca-fbf2bd2821e6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Rpv5h40Jsmo",
        "outputId": "29f39539-6d87-4ddf-a1f0-7e68ef030c2e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# spliting the train data further in validation data\n",
        "x_valid,y_valid= x_train[:5000],y_train[:5000]"
      ],
      "metadata": {
        "id": "qObstcLuJ0hL"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reassigning train data\n",
        "x_train,y_train = x_train[5000:],y_train[5000:]"
      ],
      "metadata": {
        "id": "sdInvMGKKSRu"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the range of the data ie..pixels of the image\n",
        "x_train.max()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNkGVsFcKgu1",
        "outputId": "d9ac5d13-3dd6-490f-9ebc-45f35c5ae4ec"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "255"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Without Batch Normalization"
      ],
      "metadata": {
        "id": "9nVQUs77IYDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the layers of the data\n",
        "\n",
        "Layers = [\n",
        "    tf.keras.layers.Flatten(input_shape=(28,28), name= 'InputLayer'),\n",
        "tf.keras.layers.Dense( 300, activation= 'relu', name='hiddenlayer1'),\n",
        "tf.keras.layers.Dense( 100, activation= 'relu', name='hiddenlayer2'),\n",
        "    tf.keras.layers.Dense(10 , activation='softmax' , name= 'outputlayer')\n",
        "\n",
        "]"
      ],
      "metadata": {
        "id": "4YfQwoL4MVyS"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normalizing the data\n",
        "x_train_scaled , x_valid_scaled = x_train[5000:]/255,y_train[5000:]/255\n",
        "x_test_normalized = x_test/255"
      ],
      "metadata": {
        "id": "pijFY_PhLGMl"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combining the layers\n",
        "model_classifier = tf.keras.models.Sequential(Layers)"
      ],
      "metadata": {
        "id": "BJ7gacIuN872"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_classifier.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbLX9CTROSQt",
        "outputId": "45ef6791-9006-4f3c-9363-b4d55a35c94d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " InputLayer (Flatten)        (None, 784)               0         \n",
            "                                                                 \n",
            " hiddenlayer1 (Dense)        (None, 300)               235500    \n",
            "                                                                 \n",
            " hiddenlayer2 (Dense)        (None, 100)               30100     \n",
            "                                                                 \n",
            " outputlayer (Dense)         (None, 10)                1010      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 266610 (1.02 MB)\n",
            "Trainable params: 266610 (1.02 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# defining hyperparameters for the traing of our model.\n",
        "loss=\"sparse_categorical_crossentropy\"\n",
        "optimizer =\"SGD\"\n",
        "metrics = [\"accuracy\"]\n",
        "\n",
        "model_classifier.compile(loss= loss,optimizer= optimizer, metrics= metrics)"
      ],
      "metadata": {
        "id": "UkTQfCLPO5Z7"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fitting the data to the model for training\n",
        "history= model_classifier.fit(x_train ,y_train , epochs= 30 , validation_data= (x_valid,y_valid))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xv7_j-bVPoFS",
        "outputId": "8e7cf915-d6da-44da-ed80-3232c3981e98"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "1719/1719 [==============================] - 9s 4ms/step - loss: 175820668928.0000 - accuracy: 0.1121 - val_loss: 2.3010 - val_accuracy: 0.1126\n",
            "Epoch 2/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 2.3326 - accuracy: 0.1124 - val_loss: 2.3009 - val_accuracy: 0.1126\n",
            "Epoch 3/30\n",
            "1719/1719 [==============================] - 6s 4ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1126\n",
            "Epoch 4/30\n",
            "1719/1719 [==============================] - 6s 3ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3009 - val_accuracy: 0.1126\n",
            "Epoch 5/30\n",
            "1719/1719 [==============================] - 7s 4ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3009 - val_accuracy: 0.1126\n",
            "Epoch 6/30\n",
            "1719/1719 [==============================] - 6s 3ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3008 - val_accuracy: 0.1126\n",
            "Epoch 7/30\n",
            "1719/1719 [==============================] - 6s 4ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1126\n",
            "Epoch 8/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1126\n",
            "Epoch 9/30\n",
            "1719/1719 [==============================] - 6s 3ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3009 - val_accuracy: 0.1126\n",
            "Epoch 10/30\n",
            "1719/1719 [==============================] - 6s 3ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3009 - val_accuracy: 0.1126\n",
            "Epoch 11/30\n",
            "1719/1719 [==============================] - 6s 4ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3008 - val_accuracy: 0.1126\n",
            "Epoch 12/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 2.3365 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1126\n",
            "Epoch 13/30\n",
            "1719/1719 [==============================] - 6s 3ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1126\n",
            "Epoch 14/30\n",
            "1719/1719 [==============================] - 6s 3ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1126\n",
            "Epoch 15/30\n",
            "1719/1719 [==============================] - 6s 4ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1126\n",
            "Epoch 16/30\n",
            "1719/1719 [==============================] - 6s 3ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1126\n",
            "Epoch 17/30\n",
            "1719/1719 [==============================] - 6s 3ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1126\n",
            "Epoch 18/30\n",
            "1719/1719 [==============================] - 6s 3ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1126\n",
            "Epoch 19/30\n",
            "1719/1719 [==============================] - 6s 3ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3009 - val_accuracy: 0.1126\n",
            "Epoch 20/30\n",
            "1719/1719 [==============================] - 6s 3ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3009 - val_accuracy: 0.1126\n",
            "Epoch 21/30\n",
            "1719/1719 [==============================] - 6s 3ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3009 - val_accuracy: 0.1126\n",
            "Epoch 22/30\n",
            "1719/1719 [==============================] - 6s 4ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3009 - val_accuracy: 0.1126\n",
            "Epoch 23/30\n",
            "1719/1719 [==============================] - 6s 3ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3009 - val_accuracy: 0.1126\n",
            "Epoch 24/30\n",
            "1719/1719 [==============================] - 6s 4ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3008 - val_accuracy: 0.1126\n",
            "Epoch 25/30\n",
            "1719/1719 [==============================] - 6s 4ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3008 - val_accuracy: 0.1126\n",
            "Epoch 26/30\n",
            "1719/1719 [==============================] - 6s 3ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3009 - val_accuracy: 0.1126\n",
            "Epoch 27/30\n",
            "1719/1719 [==============================] - 6s 3ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3009 - val_accuracy: 0.1126\n",
            "Epoch 28/30\n",
            "1719/1719 [==============================] - 6s 4ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3009 - val_accuracy: 0.1126\n",
            "Epoch 29/30\n",
            "1719/1719 [==============================] - 6s 3ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3008 - val_accuracy: 0.1126\n",
            "Epoch 30/30\n",
            "1719/1719 [==============================] - 6s 4ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3009 - val_accuracy: 0.1126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# saving our model for re usability\n",
        "model_classifier.save('classifier.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZxCZ9UXDUUA",
        "outputId": "42fabbdd-1dd6-4da8-dc45-3e757bf44392"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluatinmg the model\n",
        "model_classifier.evaluate(x_test , y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVr9ekKjE5et",
        "outputId": "ee3bad78-17ce-42ea-d01c-68f9d581bdb1"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 2.3011 - accuracy: 0.1135\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.3010647296905518, 0.11349999904632568]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We clearly seen that the performance of our model is poor."
      ],
      "metadata": {
        "id": "OvMchJqGIGGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# With Batch Normalization"
      ],
      "metadata": {
        "id": "jcbwAskNIg9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Layers = [\n",
        "    tf.keras.layers.Flatten(input_shape=(28,28), name= 'InputLayer'),\n",
        "tf.keras.layers.Dense( 300, activation= 'relu', name='hiddenlayer1'),\n",
        "    tf.keras.layers.BatchNormalization(),  # <----------------- batch normalization layer\n",
        "tf.keras.layers.Dense( 100, activation= 'relu', name='hiddenlayer2'),\n",
        "    tf.keras.layers.Dense(10 , activation='softmax' , name= 'outputlayer')\n",
        "\n",
        "]"
      ],
      "metadata": {
        "id": "2njwxg17FFuU"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_classifier = tf.keras.models.Sequential(Layers)"
      ],
      "metadata": {
        "id": "gUAPNnOdF2ja"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss=\"sparse_categorical_crossentropy\"\n",
        "optimizer =\"SGD\"\n",
        "metrics = [\"accuracy\"]\n",
        "\n",
        "model_classifier.compile(loss= loss,optimizer= optimizer, metrics= metrics)"
      ],
      "metadata": {
        "id": "tIFoYcyaGEdK"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history= model_classifier.fit(x_train ,y_train , epochs= 30 , validation_data= (x_valid,y_valid))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsoZj6wKGLYh",
        "outputId": "5f2e3e97-3291-49dd-8bac-1db6f8819545"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "1719/1719 [==============================] - 9s 4ms/step - loss: 0.3560 - accuracy: 0.8951 - val_loss: 0.1723 - val_accuracy: 0.9492\n",
            "Epoch 2/30\n",
            "1719/1719 [==============================] - 7s 4ms/step - loss: 0.1692 - accuracy: 0.9511 - val_loss: 0.1272 - val_accuracy: 0.9658\n",
            "Epoch 3/30\n",
            "1719/1719 [==============================] - 7s 4ms/step - loss: 0.1254 - accuracy: 0.9634 - val_loss: 0.1091 - val_accuracy: 0.9694\n",
            "Epoch 4/30\n",
            "1719/1719 [==============================] - 7s 4ms/step - loss: 0.1006 - accuracy: 0.9703 - val_loss: 0.1232 - val_accuracy: 0.9708\n",
            "Epoch 5/30\n",
            "1719/1719 [==============================] - 7s 4ms/step - loss: 0.0844 - accuracy: 0.9755 - val_loss: 0.1179 - val_accuracy: 0.9744\n",
            "Epoch 6/30\n",
            "1719/1719 [==============================] - 7s 4ms/step - loss: 0.0720 - accuracy: 0.9794 - val_loss: 0.1068 - val_accuracy: 0.9738\n",
            "Epoch 7/30\n",
            "1719/1719 [==============================] - 7s 4ms/step - loss: 0.0628 - accuracy: 0.9819 - val_loss: 0.1054 - val_accuracy: 0.9766\n",
            "Epoch 8/30\n",
            "1719/1719 [==============================] - 7s 4ms/step - loss: 0.0537 - accuracy: 0.9842 - val_loss: 0.1039 - val_accuracy: 0.9764\n",
            "Epoch 9/30\n",
            "1719/1719 [==============================] - 8s 4ms/step - loss: 0.0465 - accuracy: 0.9869 - val_loss: 0.1088 - val_accuracy: 0.9766\n",
            "Epoch 10/30\n",
            "1719/1719 [==============================] - 7s 4ms/step - loss: 0.0416 - accuracy: 0.9886 - val_loss: 0.1005 - val_accuracy: 0.9760\n",
            "Epoch 11/30\n",
            "1719/1719 [==============================] - 7s 4ms/step - loss: 0.0367 - accuracy: 0.9896 - val_loss: 0.0843 - val_accuracy: 0.9764\n",
            "Epoch 12/30\n",
            "1719/1719 [==============================] - 7s 4ms/step - loss: 0.0328 - accuracy: 0.9911 - val_loss: 0.1006 - val_accuracy: 0.9782\n",
            "Epoch 13/30\n",
            "1719/1719 [==============================] - 8s 4ms/step - loss: 0.0293 - accuracy: 0.9922 - val_loss: 0.1039 - val_accuracy: 0.9770\n",
            "Epoch 14/30\n",
            "1719/1719 [==============================] - 8s 4ms/step - loss: 0.0271 - accuracy: 0.9931 - val_loss: 0.0902 - val_accuracy: 0.9772\n",
            "Epoch 15/30\n",
            "1719/1719 [==============================] - 8s 4ms/step - loss: 0.0246 - accuracy: 0.9939 - val_loss: 0.1011 - val_accuracy: 0.9778\n",
            "Epoch 16/30\n",
            "1719/1719 [==============================] - 7s 4ms/step - loss: 0.0224 - accuracy: 0.9942 - val_loss: 0.1039 - val_accuracy: 0.9776\n",
            "Epoch 17/30\n",
            "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0196 - accuracy: 0.9954 - val_loss: 0.1015 - val_accuracy: 0.9790\n",
            "Epoch 18/30\n",
            "1719/1719 [==============================] - 7s 4ms/step - loss: 0.0192 - accuracy: 0.9953 - val_loss: 0.1196 - val_accuracy: 0.9780\n",
            "Epoch 19/30\n",
            "1719/1719 [==============================] - 8s 4ms/step - loss: 0.0177 - accuracy: 0.9956 - val_loss: 0.0816 - val_accuracy: 0.9780\n",
            "Epoch 20/30\n",
            "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0155 - accuracy: 0.9961 - val_loss: 0.1222 - val_accuracy: 0.9782\n",
            "Epoch 21/30\n",
            "1719/1719 [==============================] - 7s 4ms/step - loss: 0.0164 - accuracy: 0.9962 - val_loss: 0.1161 - val_accuracy: 0.9774\n",
            "Epoch 22/30\n",
            "1719/1719 [==============================] - 8s 4ms/step - loss: 0.0134 - accuracy: 0.9974 - val_loss: 0.1113 - val_accuracy: 0.9786\n",
            "Epoch 23/30\n",
            "1719/1719 [==============================] - 7s 4ms/step - loss: 0.0132 - accuracy: 0.9971 - val_loss: 0.1094 - val_accuracy: 0.9778\n",
            "Epoch 24/30\n",
            "1719/1719 [==============================] - 7s 4ms/step - loss: 0.0121 - accuracy: 0.9976 - val_loss: 0.1109 - val_accuracy: 0.9778\n",
            "Epoch 25/30\n",
            "1719/1719 [==============================] - 8s 4ms/step - loss: 0.0118 - accuracy: 0.9976 - val_loss: 0.1073 - val_accuracy: 0.9788\n",
            "Epoch 26/30\n",
            "1719/1719 [==============================] - 7s 4ms/step - loss: 0.0111 - accuracy: 0.9978 - val_loss: 0.1067 - val_accuracy: 0.9772\n",
            "Epoch 27/30\n",
            "1719/1719 [==============================] - 8s 4ms/step - loss: 0.0096 - accuracy: 0.9981 - val_loss: 0.1363 - val_accuracy: 0.9768\n",
            "Epoch 28/30\n",
            "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0094 - accuracy: 0.9981 - val_loss: 0.1154 - val_accuracy: 0.9778\n",
            "Epoch 29/30\n",
            "1719/1719 [==============================] - 7s 4ms/step - loss: 0.0088 - accuracy: 0.9982 - val_loss: 0.1126 - val_accuracy: 0.9790\n",
            "Epoch 30/30\n",
            "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0083 - accuracy: 0.9984 - val_loss: 0.0977 - val_accuracy: 0.9794\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_classifier.evaluate(x_test , y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKKnXav1GRwK",
        "outputId": "0910caf9-96fc-4f2d-d85d-76eda3cefffc"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0808 - accuracy: 0.9803\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.08084678649902344, 0.9803000092506409]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a conclusion, we clearly seen that how our model accuracy will increse after using the batch normalization. This shows the significance of this technique."
      ],
      "metadata": {
        "id": "QI6OSP0SI0bm"
      }
    }
  ]
}